# OpenStack 2024.2 All-in-One Installation with Kolla-Ansible on Ubuntu 24.04 LTS

This guide walks through deploying OpenStack **2024.2** (stable/Dalmatian release) on a single Ubuntu 24.04 VM using **Kolla-Ansible**. We will set up an All-In-One (AIO) environment in VirtualBox with one controller/compute node. All OpenStack services will run in Docker containers orchestrated by Kolla-Ansible. The configuration includes Designate for DNS service with a local Bind9 server.

## 1. Virtual Machine Setup and Networking

Consider disabling Hyper-V on your Windows machine and upgrading VirtualBox to latest release - this should remove CPU race conditions between virtualization engines.
Type `Turn Windows features on or off` into Windows Search Bar, uncheck **Virtual Mchine Platform** and **Windows Hypervisor Platform**, confirm and reboot the system. 

Before installation, verify configuration of **Host-only Network** and **NAT Network** in VirtualBox Network Tool, create networks if necessary:

- **Host-only Network**: Host-only Networks, Configure Adapter Manually, IPv4 Address 10.0.0.1, IPv4 Network Mask 255.255.255.0, DHCP Server - uncheck Enable Server.
- **NAT Network**: NAT Networks, General Options, Name "provider", IPv4 Prefix 203.0.113.0/24, uncheck Enable DHCP and Enable IPv6.

Create a VirtualBox VM with the following specs and network layout:

- **Virtual Machine name:** `os-dalmatian-aio`, if you change it mind the commands below, as they may reference this name.
- Check **Skip Unattended Installation**
- in **Hardware** section set: Base Memory to 16 GB, Processors to 16
- in **Hard Disk** section set primary disk size to 100 GB

Click on **Settings** for newly created VM and adjust following options:

- **System** -> **Processor**: check **Enable PAE/NX**
- **Storage**:  
  - Disk1: 100 GB (for OS and OpenStack containers)  
  - Disk2: 120 GB (for Cinder LVM volumes) - add a new disk on SATA controller
- **Network**:  
  - `Adapter 1`: Attach to **NAT** or Bridged for Internet access (DHCP enabled). This provides outbound connectivity for package installation and container pulls.  
  - `Adapter 2`: Enable and attach to a **Host-Only Network** (VirtualBox Host Network) and assign it a static IP (e.g. **10.0.0.11/24**). This will be the OpenStack **management network** interface, used for Horizon (dashboard) access and internal API communication. Create the Host Network in VirtualBox Network Manager window before creating the VM, if necessary. 
  - `Adapter 3`: Enable and attach to a **NAT Network** with name `provider`, configured with subnet **203.0.113.0/24**. This will serve as the Neutron **provider network** for external (Floating IP) connectivity. Set **Promiscuous Mode** to `Allow All`. Do **not** configure an IP on `enp0s9` inside the VM (it will be managed by Neutron). Create the NAT Network in VirtualBox Network Manager window before creating the VM, if necessary.  

**Networking details:** In this setup, `enp0s8` (10.0.0.11) is an isolated network between your host and VM (for SSH and accessing Horizon), and `enp0s9` is a provider network that instances will attach to for external access. The NAT network 203.0.113.0/24 is a VirtualBox network that will NAT instance traffic to the real internet. We will assign Floating IPs from this range to instances. (203.0.113.0/24 is a documentation IP range reserved for examples ([Provider network — Installation Guide  documentation](https://docs.openstack.org/install-guide/launch-instance-networks-provider.html#:~:text=Example)).)

**Enable Nested Virtualization:** this allows use of KVM and 'real' Instances in the Lab:

```bash
cd "C:\Program Files\Oracle\VirtualBox"
.\VBoxManage modifyvm os-dalmatian-aio --nested-hw-virt on
cd ~
```

Verify in Settings of `os-dalmatian-aio`, System->Processor where you see `Enable Nested VT-x/AMD-V` checked.

## 2. Ubuntu 22.04 Initial Configuration

Start the `os-dalmatian-aio` VM and perform a **minimal** installation of Ubuntu Server 22.04 on the VM, use `English` as main language. Install system on 100GB disk (!), LVM on root disk is not required for the LAB purposes. Configure only first Network Interface with DHCP only, all other Network Interface will be configured later. Configure default user with **`openstack`** username and password of your choice. The course assumes this username in exercises. Set hostname to `controller`. Check `Install OpenSSH server` option, it's required for the LAB. 
After installation and reboot, login as `openstack` and configure the system as follows:

**2.1 Create OpenStack User with Sudo:** Give this user password-less sudo privileges for convenience:

```bash
echo "openstack ALL=(ALL:ALL) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/openstack
```

Install Nano text editor to allow easy text file edits:

```bash
sudo apt install -y nano
```

**2.2 Configure Network Interfaces:** Ubuntu 22.04 uses Netplan for networking. Edit Netplan config (e.g. `/etc/netplan/01-netcfg.yaml`) to set up the NICs:

- `enp0s3`: DHCP (for NAT internet access).
- `enp0s8`: Static IP **10.0.0.11/24** (no gateway on this interface, since the host-only network is non-routable).
- `enp0s9`: Leave unconfigured (no IP) so it can be used as a provider network interface by Neutron ([OpenStack Open Source Cloud Computing Software » Message: [openstack-dev] Problem when deploying Openstack with Kolla ](https://lists.openstack.org/pipermail/openstack-dev/2018-May/130703.html#:~:text=%3E%20,enp0s9)).

Create the Netplan YAML file `/etc/netplan/01-netcfg.yaml` (with `nano` editor): 

```yaml
network:
  version: 2
  renderer: networkd
  ethernets:
    enp0s3:
      dhcp4: true
    enp0s8:
      dhcp4: false
      addresses: [10.0.0.11/24]
      routes: []          # no default route via enp0s8
      nameservers:
        addresses: [8.8.8.8, 8.8.4.4]   # use public DNS for general name resolution
    enp0s9:
      dhcp4: false        # no IP (Neutron will use this interface)
      optional: true
```

Apply the Netplan config: 

```bash
sudo netplan apply
sudo rm -f /etc/netplan/50-cloud-init.yaml
```

After this, you should be able to SSH to the VM at 10.0.0.11 from the host, and the VM should still have internet via enp0s3:

```bash
ssh openstack@10.0.0.11
```

If you reinstall you may need to remove old SSH host keys on your VirtualBox host:

```bash
ssh-keygen -R 10.0.0.11
```

**2.3 Disable Cloud-Init Network Management:** Ubuntu cloud images may use **cloud-init** to manage network config. Since we manually configured networking, we can disable cloud-init to prevent it from overwriting our settings on reboot:

```bash
sudo touch /etc/cloud/cloud-init.disabled
```

This ensures cloud-init will not reset our network or SSH keys on reboot.

**2.4 Disable Automatic Updates:** Ubuntu enables unattended upgrades by default. We disable this to avoid package updates interfering with our OpenStack deployment. You can either reconfigure or remove the unattended-upgrades package ([apt - Disable automatic updates in 22.04.1 - Ask Ubuntu](https://askubuntu.com/questions/1437070/disable-automatic-updates-in-22-04-1#:~:text=Two%20additional%20ways%20to%20disable,unattended%2Fautomatic%20updates%2Fupgrades)):

```bash
sudo apt remove -y unattended-upgrades
```

This stops automatic package updates from running in the background ([apt - Disable automatic updates in 22.04.1 - Ask Ubuntu](https://askubuntu.com/questions/1437070/disable-automatic-updates-in-22-04-1#:~:text=sudo%20dpkg)). (You will manually update as needed.)

**2.5 Update APT and Install Basics:** Update system packages and install required utilities:

```bash
sudo apt update && sudo apt upgrade -y
sudo apt install -y bridge-utils cpu-checker libvirt-clients libvirt-daemon qemu-kvm
sudo apt install -y python3-dev python3-venv git libffi-dev gcc libssl-dev libdbus-glib-1-dev vim net-tools htop
```

These include Python tools and editors.

**2.6 Prepare the Second Disk for Cinder LVM:** The 120GB disk (`/dev/sdb` in the VM) will be used for Cinder volumes via LVM. Partitioning is optional – we can use the whole disk as a physical volume. Set up LVM:

```bash
# Create LVM physical volume
sudo pvcreate /dev/sdb
# Create a volume group named cinder-volumes
sudo vgcreate cinder-volumes /dev/sdb
```

Kolla-Ansible expects a volume group **cinder-volumes** to exist for the LVM backend. **Note:** This will erase any existing data on /dev/sdb.

Verify volume group:

```bash
sudo vgs
```

You should see **cinder-volumes** listed.

Also, Ubuntu requires a special filesystem **configfs** to be mounted for iSCSI to function inside containers. Mount it now and persist it (this allows the Cinder volume service to export volumes via iSCSI):

```bash
echo "configfs /sys/kernel/config configfs defaults 0 0" | sudo tee -a /etc/fstab
sudo mount -a
```

This mounts the configfs at `/sys/kernel/config` (needed by the iSCSI daemon) ([Cinder - Block storage — kolla-ansible 19.1.0.dev183 documentation](https://docs.openstack.org/kolla-ansible/latest/reference/storage/cinder-guide.html#:~:text=For%20Ubuntu%20and%20LVM2%2FiSCSI%C2%B6)).

**2.7 Install Required System Dependencies:** Kolla-Ansible will manage most dependencies, but we ensure a few are present:

- **Python DBus library:** The Kolla-Ansible playbooks use `dbus` for systemd interactions, which requires the `python3-dbus` package on the host. Install it now to avoid precheck errors ([Bug #2021585 “Bootstrap-server misses dbus” : Bugs : ansible-collection-kolla](https://bugs.launchpad.net/bugs/2021585#:~:text=kolla,with%20prechecks%20and%20deployment%20itself)):

```bash
sudo apt install -y python3-dbus
```

With the base system configured, reboot to ensure network and fstab changes take effect, then proceed as the `openstack` user for the next steps.

Consider shuting down the VM now to make a snapshot allowing easy restart of LAB installation without a need of full reinstall.

## 3. Install Kolla-Ansible and Ansible

We will use Kolla-Ansible (OpenStack official deployment tool) to deploy OpenStack in Docker containers. Kolla-Ansible is a Python package that can be installed via pip. 
Create a Python virtual environment:

```bash
cd ~
python3 -m venv --system-site-packages ~/kolla-ansible-venv
source ~/kolla-ansible-venv/bin/activate
pip install -U pip
```

Rember to reactivate the virtual environment every time to reboot machine and login.

**3.1 Install Kolla-Ansible (2024.2):** Install the Kolla-Ansible package corresponding to OpenStack 2024.2. At the time of writing, the stable release is **kolla-ansible 19.2.0** (which targets OpenStack 2024.2):

```bash
pip install kolla-ansible==19.2.0
```

This will install Kolla-Ansible and its Python dependencies. Once done, verify the version:

```bash
kolla-ansible --version
# Should show kolla-ansible 19.2.0 (or similar)
```

**3.2 Install Ansible Galaxy Dependencies:** Kolla-Ansible requires some Ansible roles from Galaxy (for things like configuring haproxy, etc.). Install them by running:

```bash
kolla-ansible install-deps
```

This uses the Kolla-Ansible utility to download necessary Ansible roles (you may see it installing roles for **ceph**, etc.) ([How to Implement an OpenStack-Based Private Cloud with Kolla-Ansible - Part 1 - Superuser](https://superuser.openinfra.org/articles/how-to-implement-an-openstack-based-private-cloud-with-kolla-ansible-part-1/#:~:text=5)).

**3.3 Copy Configuration and Inventory Templates:** Kolla-Ansible provides example configuration files. Copy the default configuration directory to `/etc/kolla` and the inventory for all-in-one deployment:

```bash
sudo mkdir -p /etc/kolla
sudo chown $USER:$USER /etc/kolla

# Copy example config files
cp -r  ~/kolla-ansible-venv/share/kolla-ansible/etc_examples/kolla/* /etc/kolla/

# Copy all-in-one inventory to current directory for editing
cp  ~/kolla-ansible-venv/share/kolla-ansible/ansible/inventory/all-in-one .
```

Now you should have `/etc/kolla/globals.yml` and `/etc/kolla/passwords.yml` ready to edit, and an `all-in-one` inventory file in your working directory.

**3.4 Generate Passwords:** Kolla stores all service passwords in `/etc/kolla/passwords.yml`. Generate random passwords for all entries:

```bash
kolla-genpwd
```

This will fill in `passwords.yml` with random values for all OpenStack services.

## 4. Configure Kolla-Ansible (globals.yml)

Open `/etc/kolla/globals.yml` in your editor and set the following parameters to match our environment (below first line with `---`). We will disable high availability (since this is a one-node deployment) and enable the required OpenStack services. Below is a summary of key settings to change or add:

- **Networking Interfaces:**  
```yaml
# Configure Network Interfaces
network_interface: "enp0s8"
api_interface: "enp0s8"
neutron_external_interface: "enp0s9"
dns_interface: "enp0s8"
```  
  **Explanation:** `network_interface` is the main interface for internal API endpoints and cluster traffic. We set it to `enp0s8` (10.0.0.11) which has our management IP ([OpenStack Open Source Cloud Computing Software » Message: [openstack-dev] Problem when deploying Openstack with Kolla ](https://lists.openstack.org/pipermail/openstack-dev/2018-May/130703.html#:~:text=%3E%20,%3E%20these%20can)). We also set `api_interface` to the same (this can default to network_interface, but we ensure it's enp0s8). The `neutron_external_interface` is the NIC that will be used for external/provider networks – we set this to `enp0s9` (and remember, enp0s9 has no IP on the host) ([OpenStack Open Source Cloud Computing Software » Message: [openstack-dev] Problem when deploying Openstack with Kolla ](https://lists.openstack.org/pipermail/openstack-dev/2018-May/130703.html#:~:text=%3E%20,enp0s9)). We also set `dns_interface` to enp0s8 so that Designate’s DNS service (mdns/bind) will listen on the management network ([Designate - DNS service — kolla-ansible 19.1.0.dev183 documentation](https://docs.openstack.org/kolla-ansible/latest/reference/networking/designate-guide.html#:~:text=Important)).

- **OpenStack Release and Base Settings:**  
```yaml
# Configure OpenStack release and base settings
openstack_release: "2024.2"
kolla_base_distro: "ubuntu"
kolla_install_type: "source"
```  
  We specify the OpenStack version (2024.2) to ensure Kolla pulls the correct container images for this release. This may already be the default, but it’s good to be explicit.

- **Disable High Availability:**  
```yaml
# Disable High Availability
enable_haproxy: "no"
enable_keepalived: "no"
enable_mariadb_proxy: "no"
enable_proxysql: "no"
enable_rabbitmq_cluster: "no"
kolla_internal_vip_address: "10.0.0.11"
default_container_dimensions:
  cpuset_cpus: "1"
```  
  In an AIO setup, we don’t need HAProxy or Keepalived (used for HA proxying and virtual IP management across multiple nodes). Disabling HA saves resources and complexity ([Kolla Ansible Workshop - HackMD](https://hackmd.io/s/BycDsf4OZ#:~:text=kolla_base_distro%3A%20,deploy%60%60%60%20%60%60%60%20source)). Kolla will have services bind directly to the host interface.

- **Enable Core OpenStack Services:** Ensure the essential services are enabled. Some core services are enabled by default, but we will explicitly list them for clarity:
```yaml
# Enable Core OpenStack Services
enable_keystone: "yes"
enable_glance: "yes"
enable_nova: "yes"
nova_compute_virt_type: "kvm"
enable_neutron: "yes"
enable_horizon: "yes"
enable_placement: "yes"
enable_cinder: "yes"
```
  Keystone (identity), Glance (image), Nova (compute), Neutron (networking), Placement, Cinder (block storage), and Horizon (dashboard) should all be enabled. (Kolla’s default `enable_openstack_core` flag usually covers these, but we list them to be sure.)

- **Enable Additional Services:** Enable the optional services requested:
```yaml
# Enable Additional Services
enable_heat: "yes"
enable_ceilometer: "yes"
enable_aodh: "yes"
enable_gnocchi: "yes"
enable_designate: "yes"
```
  This turns on  **Heat** (orchestration), **Ceilometer** (telemetry data collection), **Aodh** (alarms), **Gnocchi** (time-series database for Ceilometer metrics), and **Designate** (DNS service). By default these are not all enabled, so we must specify them. Enabling Designate will also pull in a Bind9 container for DNS.

- **Cinder LVM Backend:**  
```yaml
# Configure Cinder LVM Backend
enable_cinder_backend_lvm: "yes"
cinder_volume_group: "cinder-volumes"
```  
  Enable the LVM backend for Cinder ([Cinder - Block storage — kolla-ansible 19.1.0.dev183 documentation](https://docs.openstack.org/kolla-ansible/latest/reference/storage/cinder-guide.html#:~:text=match%20at%20L205%20In%20order,globals.yml)). We already created the volume group `cinder-volumes`, so we reference it here. This will deploy Cinder Volume and ISCSI services using LVM. (The `cinder_volume_group` may default to "cinder-volumes", but we set it explicitly.)

- **Designate (DNS) Configuration:**  
  In addition to `enable_designate: "yes"`, add:
```yaml
# Configure Designate
neutron_dns_domain: "example.org."
designate_backend: "bind9"
designate_ns_record: ["ns1.example.org."]
designate_enable_notifications_sink: "yes"
designate_forwarders_addresses: ["8.8.8.8", "8.8.4.4"]
```
  **Explanation:** We set `neutron_dns_domain` to **example.org.** (note the trailing dot) ([Designate - DNS service — kolla-ansible 19.1.0.dev183 documentation](https://docs.openstack.org/kolla-ansible/latest/reference/networking/designate-guide.html#:~:text=Enable%20Designate%20service%20in%20)). This domain will be used for internal DNS names of instances. It **must not** be "openstacklocal" (the default) and must end with a dot ([Designate - DNS service — kolla-ansible 19.1.0.dev183 documentation](https://docs.openstack.org/kolla-ansible/latest/reference/networking/designate-guide.html#:~:text=Important)). We use "example.org." as an example. We choose the Bind9 backend for Designate (which is the default) – this means Designate will manage DNS zones using a Bind9 server. By default, Kolla will run a containerized Bind9 on the controller. We also specify an NS record (nameserver) for our domain (e.g., ns1.example.org.). Finally, we configure DNS forwarders: by setting `designate_forwarders_addresses` to **8.8.8.8**, we ensure our Bind9 will forward queries for any DNS zones it’s not authoritative for (i.e., external domains) to Google’s DNS ([Charmhub | Deploy Designate Bind using Charmhub - The Open Operator Collection](https://charmhub.io/designate-bind/configurations#:~:text=forwarders%20)). This makes the Bind9 server a caching DNS resolver for our instances. Without this, the Bind9 server might not resolve external domains. 

  *Note:* The forwarders setting uses a single IP here, but you could list multiple (e.g. `"8.8.8.8;8.8.4.4"`). Kolla-Ansible will insert this into the bind named.conf if provided (to avoid an empty forwarders block) ([Fixes bind9 restarting when no designate_forwarders_addresses · fae046ece8 - kolla-ansible - OpenDev: Free Software Needs Free Tools](https://opendev.org/openstack/kolla-ansible/commit/fae046ece81b5fb1d4fce635fc97574eeb8693ab?style=unified&whitespace=ignore-eol&show-outdated=#:~:text=%60%20recursion%20,)).

- **Horizon Setting for non-default services:**

```yaml
# Enable Horizon for non-default services
enable_horizon_designate: "yes"
enable_horizon_heat: "yes"
```

After making these changes, save `globals.yml`.

**4.1 (Re)Check Configuration:** It’s a good idea to double-check that `/etc/kolla/globals.yml` reflects all the changes (especially `network_interface`, `neutron_external_interface`, service enables, etc.). Also verify that `all-in-one` inventory has the IP of your host if needed (in all-in-one, by default it uses `localhost`). The provided `all-in-one` inventory typically contains `localhost       ansible_connection=local` which is fine for a single-node deploy on the same machine.

## 5. Deploy OpenStack with Kolla-Ansible

With configuration in place, we can run Kolla-Ansible playbooks to set up OpenStack. We will run the following steps in order:

1. **Bootstrap Servers** – install base packages (like Docker) on the target node.  
2. **Prechecks** – verify all prerequisites are met.  
3. **Deploy** – actually deploy all containers and services.  
4. **Post-deploy** – finalize the deployment (e.g., generate admin OpenRC file).  

Make sure you are in the directory containing your `all-in-one` inventory file and that your virtualenv (if used) is activated (so the `kolla-ansible` command is available). All commands should be run as the **openstack** user (with sudo where noted).

**5.1 Bootstrap the Server:** Run Kolla-Ansible bootstrap playbook:

```bash
kolla-ansible bootstrap-servers -i all-in-one
```

This will use Ansible to prepare the node. It installs Docker and configures basics like chrony for NTP, etc. Watch for any errors. This step requires sudo because it makes changes to the system.

   - *Troubleshooting:* The bootstrap process should install Docker CE. If it fails or if you prefer to install Docker manually, ensure Docker Engine (20.x or later) is installed and running. 
   - Ignore errors related to missing **ufw**
   
Also, since we’re using the Docker driver, confirm that the **Docker Python SDK** is installed for Ansible. If not already present, install it with: 

```bash
pip install docker
```

(If the prechecks later complain with *“No module named docker”*, this is the fix ([python - Kolla-ansible openstack: Docker version failure - Stack Overflow](https://stackoverflow.com/questions/59832284/kolla-ansible-openstack-docker-version-failure#:~:text=docker,)).)

Fix the Python `requests` module version to avoid `prechecks` issues:

```bash
pip uninstall requests
pip install requests==2.31.0
```

**5.2 Run Pre-Deployment Checks:** Next, run Kolla-Ansible prechecks to validate the configuration:

```bash
kolla-ansible prechecks -i all-in-one
```

This will do a series of checks (disk space, kernel parameters, config sanity, etc.) Ensure it finishes with “SUCCESS”. If any errors arise, address them:

   - **Common precheck issues:** Missing Python modules (like dbus or docker) or missing volume group. For instance, if you forgot to install `python3-dbus`, the precheck will fail at *"Checking dbus-python package"* ([Bug #2021585 “Bootstrap-server misses dbus” : Bugs : ansible-collection-kolla](https://bugs.launchpad.net/bugs/2021585#:~:text=kolla,with%20prechecks%20and%20deployment%20itself)). Make sure `python3-dbus` is installed on the host (as we did in step 2.7). If the Cinder LVM backend is enabled but no volume group is found, prechecks will warn you. We created `cinder-volumes` VG already, so that should pass. 

   - If any **“FAILED”** status appears, read the message and fix the issue before proceeding.

**5.3 Deploy OpenStack Services:** Now run the main deploy playbook:

```bash
kolla-ansible deploy -i all-in-one
```

This step will take some time. Kolla-Ansible will pull Docker images for all enabled services and start the containers with proper configurations. All OpenStack services (Keystone, Glance, Nova, Neutron, etc.) will be set up as Docker containers. You will see Ansible output for each role/service being deployed ([How to Implement an OpenStack-Based Private Cloud with Kolla-Ansible - Part 1 - Superuser](https://superuser.openinfra.org/articles/how-to-implement-an-openstack-based-private-cloud-with-kolla-ansible-part-1/#:~:text=3,containers%20for%20each%20OpenStack%20service)). 

Expect this to run for 10-30 minutes depending on internet speed and VM performance (as it has to download many container images and start them). 

Once completed, you should see a message indicating the playbook run is finished without fatal errors. You can check running containers with `sudo docker ps` – you should see containers for keystone, mysql, rabbitmq, horizon, neutron, nova, etc. (Do not worry if you don’t immediately see all; some containers exit after doing one-time setup, e.g., kolla_setup_* containers.)

**5.4 Post-Deployment Tasks:** After a successful deploy, run the post-deploy playbook to finalize setup:

```bash
kolla-ansible post-deploy -i all-in-one
```

This will generate an **admin OpenStack RC file** (`/etc/kolla/admin-openrc.sh`) containing environment variables to use the OpenStack CLI ([Kolla Ansible Workshop - HackMD](https://hackmd.io/s/BycDsf4OZ#:~:text=%60external_interface%20enp3s0f0%60%20%21%5B%5D%28https%3A%2F%2Fi.imgur.com%2FaQbEld4.jpg%29%20%60%20,https%3A%2F%2Fi.imgur.com%2FvV7zF6L.png%29%20OpenStack%20%E8%AA%8D%E8%AD%89%E8%B3%87%E8%A8%8A%E8%AB%8B%E5%8F%83%E8%80%83%20%60%2Fetc%2Fkolla%2Fadmin)). It may also set up default networks if that is configured (though by default, Kolla doesn’t create external network – we will do that manually).

Once this finishes, source the admin credentials:

```bash
source /etc/kolla/admin-openrc.sh
```

Now you can use the `openstack` CLI. For example, try `openstack service list` to see all registered OpenStack services. This verifies Keystone is working and services are registered (you should see entries for identity, compute, network, etc.).

## 6. Post-Deployment Configuration and Verification

At this point, OpenStack should be up and running. We’ll do a few additional configurations for Designate and Neutron, and verify access to Horizon and other services.

**6.1 Verify Horizon Dashboard:** The Horizon container should be running and bound to the management IP (10.0.0.11) on port 80 by default. From your host machine, open a browser and navigate to **http://10.0.0.11/**. You should see the OpenStack Dashboard login page ([How to Implement an OpenStack-Based Private Cloud with Kolla-Ansible - Part 1 - Superuser](https://superuser.openinfra.org/articles/how-to-implement-an-openstack-based-private-cloud-with-kolla-ansible-part-1/#:~:text=within%20the%20OpenStack%20environment)). Log in with the **admin** account. The password for admin can be found in `/etc/kolla/passwords.yml` (look for `keystone_admin_password`). Alternatively, since we sourced the `admin-openrc.sh`, the password is available as the environment variable `$OS_PASSWORD`. 

  - Username: `admin`  
  - Password: (from `admin-openrc.sh` or `passwords.yml`)  

  > **Tip:** OpenStack generates a random admin password when you ran `kolla-genpwd`. To retrieve it, you can run: `grep keystone_admin_password /etc/kolla/passwords.yml`. Use that value to log in.

After logging in, you should see the Horizon dashboard. This confirms that Keystone, Horizon, and their integration are functioning.

**6.2 Create External Network and Router:** Kolla-Ansible does not automatically create the external provider network or a default project network. We will create an external network to represent the 203.0.113.0/24 provider network on `enp0s9`, so that floating IPs can be allocated. Execute the following OpenStack CLI commands (as admin):

```bash
# 1. Create an external provider network (flat network on physnet1 by default in Kolla)
openstack network create --share --external \
  --provider-network-type flat --provider-physical-network physnet1 \
  provider-net

# 2. Create a subnet on this network (203.0.113.0/24 range)
openstack subnet create --network provider-net --allocation-pool start=203.0.113.100,end=203.0.113.200 \
  --dns-nameserver 8.8.8.8 --gateway 203.0.113.1 --subnet-range 203.0.113.0/24 provider-net-subnet
```

We named the external network **provider-net** and mapped it to `physnet1` flat network (Kolla by default maps `neutron_external_interface` to `physnet1`). We allocated a portion of the subnet for floating IPs (100-200) and set the gateway to 203.0.113.1 ([Provider network — Installation Guide  documentation](https://docs.openstack.org/install-guide/launch-instance-networks-provider.html#:~:text=The%20provider%20network%20uses%20203,4%20as%20a%20DNS%20resolver)) (the NAT network’s gateway). We also specified a DNS resolver (8.8.8.8) for instances.

Next, create an internal (tenant) network for testing (or use Horizon to do this):

```bash
# Create a demo project network (or you can use the admin project)
openstack network create demo-net
openstack subnet create --network demo-net --subnet-range 10.10.10.0/24 --dns-nameserver 8.8.8.8 demo-subnet

# Create a router and connect the demo subnet to the external net
openstack router create demo-router
openstack router set demo-router --external-gateway provider-net
openstack router add subnet demo-router demo-subnet
```

This sets up a simple tenant network (10.10.10.0/24) and a router with an interface on that subnet and a gateway to the provider network. At this point, instance traffic from the demo-net can be SNATed out to the external network by the router, and you can allocate floating IPs from provider-net to instances.

**6.3 Launch a Test Instance:** Upload a test image and try booting an instance to verify Nova, Neutron, Cinder:

```bash
# Upload a CirrOS test image
openstack image create --file cirros-0.6.1-x86_64-disk.img --disk-format qcow2 --container-format bare cirros

# Create a flavor
openstack flavor create --ram 256 --disk 1 --vcpus 1 m1.tiny

# Boot an instance on the demo-net
openstack server create --flavor m1.tiny --image cirros --network demo-net test-vm
```

Once the instance is ACTIVE, allocate a floating IP:

```bash
openstack floating ip create provider-net
openstack server add floating ip test-vm <floating_ip_address>
```

Use the floating IP assigned (which will be in the 203.0.113.X range, e.g., 203.0.113.101) to test connectivity. Because the VirtualBox NAT Network is used, you might not directly ping this IP from your host (VirtualBox may not route it directly). However, the VM itself should have internet access. You can log into the instance via the Horizon console or by creating a security group rule for ICMP/SSH and ping out to verify networking.

**6.4 Verify Cinder LVM and Volume Backup:** In Horizon or CLI, create a volume and attach it to the instance, and also test the volume backup:

- Create a volume (e.g., 1 GB) and ensure it becomes **Available** ([Cinder - Block storage — kolla-ansible 19.1.0.dev183 documentation](https://docs.openstack.org/kolla-ansible/latest/reference/storage/cinder-guide.html#:~:text=Create%20a%20volume%20as%20follows%3A)). This volume will be backed by the LVM volume group on `/dev/sdb`. Check `sudo lvs` on the host to see the logical volume created.
- Attach the volume to the `test-vm` instance and verify inside the VM that a new disk (e.g., /dev/vdb) appears.
- If **Cinder Backup** is enabled, you can create a volume backup (which by default stores backups in Swift or local filesystem depending on config – Kolla might use Swift if enabled, or just store in the same volume group). Ensure the backup operation completes, indicating the cinder-backup service is functional.

**6.5 Configure and Test Designate (DNS):** Since we enabled Designate, let’s set it up:

- Ensure the **Designate central** and **bind9** containers are running. The bind9 DNS service should be listening on the `dns_interface` (10.0.0.11). We configured it to forward unknown queries to 8.8.8.8, so it can resolve external domains as well.
- Create a DNS zone in Designate for the domain we chose (example.org):

  ```bash
  openstack zone create --email admin@example.org example.org.
  ```

  This creates a DNS zone that Designate will manage. The trailing dot in the zone name is important. For example, if the command returns a zone ID, note it.

- Now we can configure Neutron to integrate with Designate. We already set `neutron_dns_domain = example.org.` in globals, which means any port/instance with a DNS name set will get a DNS record in the **example.org** zone ([Designate - DNS service — kolla-ansible 19.1.0.dev183 documentation](https://docs.openstack.org/kolla-ansible/latest/reference/networking/designate-guide.html#:~:text=Neutron%20and%20Nova%20Integration%C2%B6)). Test this by booting a new instance with a `--dns-name`:

  ```bash
  openstack server create --flavor m1.tiny --image cirros --network demo-net --dns-name testvm dns-test-vm
  ```

  If everything is configured, once this VM is ACTIVE, Designate should have created a DNS record **testvm.example.org** pointing to the VM’s fixed IP. You can verify with:

  ```bash
  openstack recordset list example.org.
  ```

  You should see an A record for *testvm*. Also, from within the demo-net (or via the VM’s console), try querying the DNS:
  
  ```bash
  dig @10.0.0.11 testvm.example.org
  ```
  
  It should return the VM’s IP. Likewise, try resolving an external name via our Bind9:
  
  ```bash
  dig @10.0.0.11 openstack.org
  ```
  
  This should succeed (Bind9 forwards the query to 8.8.8.8 as configured, since it’s not authoritative for openstack.org). This confirms Designate and our local DNS forwarding are working.

**6.6 Telemetry (Ceilometer/Gnocchi/Aodh):** The telemetry services (if enabled) will collect metrics and allow setting alarms:

- Gnocchi should be running (as a metric storage). Ceilometer agents will publish metrics like CPU utilization, etc. to Gnocchi. You can verify Gnocchi by listing metric status (beyond scope for this guide, but ensure the containers are running: `gnocchi-api`, `gnocchi-metricd`).
- Aodh allows creating alarms on these metrics. You can test this by creating an alarm (using openstack CLI or Horizon’s Alarming panel) on an instance metric and verifying it triggers.

All OpenStack services specified are now deployed and should be functional. You can manage the cloud via Horizon or CLI. 

## 7. Conclusion

You have a fully functional OpenStack 2024.2 single-node cloud running in VirtualBox. All major components (Keystone, Glance, Nova, Neutron, Cinder, Heat, Ceilometer, Gnocchi, Aodh, Designate, Horizon) are installed in Docker containers via Kolla-Ansible. The Neutron Linuxbridge agent is used for networking, with `enp0s9` serving as a provider network for external connectivity (203.0.113.0/24 Floating IPs). Designate is integrated with Neutron to provide DNS names for instances in the **example.org** domain, backed by a local Bind9 which forwards external queries to 8.8.8.8. Cinder uses LVM on the second virtual disk for volume storage, and the cinder-backup service is enabled for volume backups. 

This lab setup can be used for testing OpenStack features in a contained environment. Since no high availability is configured, it’s not for production use, but it’s perfect for learning and experimentation. 

Feel free to explore the containers (`docker ps -a`) and logs in `/var/log/kolla/` to troubleshoot or understand the deployment. You can now launch instances, attach volumes, allocate floating IPs, experiment with Heat templates, monitor metrics in Gnocchi, and use Designate to resolve instance hostnames.

Happy OpenStacking!