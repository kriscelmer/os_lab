# OpenStack 2024.2 All-in-One Installation with Kolla-Ansible on Ubuntu 24.04 LTS

This guide walks through deploying OpenStack **2024.2** (stable/Dalmatian release) on a single Ubuntu 24.04 VM using **Kolla-Ansible**. We will set up an All-In-One (AIO) environment in WMware Workstation Pro 17 with one controller/compute node. All OpenStack services will run in Docker containers orchestrated by Kolla-Ansible. The configuration includes Designate for DNS service with a local Bind9 server.

## 1. Virtual Machine Setup and Networking

Consider disabling Hyper-V on your Windows machine and upgrading VMware Workstation Pro to latest release - this should remove CPU race conditions between virtualization engines.
Type `Turn Windows features on or off` into Windows Search Bar, uncheck **Virtual Mchine Platform** and **Windows Hypervisor Platform**, confirm and reboot the system. 
Disable **Core Isolation** in **Windows Security**, if this is allowed in your PC.

Before installation, verify configuration of **Host-only Network** VMware Workstation Pro **Virtual Network Editor**, create networks if necessary:

- **Host-only Network**: Host-only Network, IPv4 Address 10.0.0.1, IPv4 Network Mask 255.255.255.0, DHCP Server disabled.

Create a VM with the following specs and network layout:

- in **New Virtual Machine Wizard** select **Custom (advanced)**
- in **Guest Operating System Installation** choose **Installer disk image file (iso)** and select Ubuntu server 24.04.x ISO image
- **Virtual machine name:** `os-dalmatian-aio`
- Number of processor set to 8 and number of cores per processor set 1
- Set Memory for the Virtual Machine to exactly 16 GB
- in Network Type select **Use network address translation (NAT)**
- Select recommended (default) I/O Controller Type and DIsk Type, choose **Create a new virtual disk** and set Maxim disk size to 100GB, choose **Store virtual disk as a single file**
- Review settings and uncheck **Power on this virtual machine after creation** and click **Finish**

Modify Virtual Machine Hardware Settings by clicking **Edit virtual machine settings** and:

- click on **Processors** and check all options in **Virtualization engine**
- Add **Hard Disk**: choose recommended Disk Type, choose **Create a new virtual disk**, set size to 120GB and choose **Store virtual disk as a single file**
- Add **Network Adapter**: choose **Custom: Specific virtual network** and select **VMnet1 (Host-only)**, which you have just created
- Add another **Network Adapter**: choose **NAT: Used to share the host's IP address**
- Remove **USB Controller** and **Sound card**

Start the `os-dalmatian-aio` VM and perform a **minimal** installation of Ubuntu Server 24.04 on the VM, use `English` as main language. Install system on 100GB disk (!), LVM on root disk is not required for the LAB purposes. Configure only first Network Interface with DHCP only, all other Network Interface will be configured later. Configure default user with **`openstack`** username and password of your choice. The course assumes this username in exercises. Set hostname to `controller`. Check `Install OpenSSH server` option, it's required for the LAB. 
After installation and reboot, login as `openstack` and configure the system as follows:

**2.1 Create OpenStack User with Sudo:** Give this user password-less sudo privileges for convenience:

```bash
echo "openstack ALL=(ALL:ALL) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/openstack
```

Install Nano text editor to allow easy text file edits:

```bash
sudo apt install -y nano
```

**2.2 Configure Network Interfaces:** Ubuntu 24.04 uses Netplan for networking. Edit Netplan config (e.g. `/etc/netplan/01-netcfg.yaml`) to set up the NICs:

- `ens32`: DHCP (for NAT internet access).
- `ens33`: Static IP **10.0.0.11/24** (no gateway on this interface, since the host-only network is non-routable).
- `ens34`: Leave unconfigured (no IP) so it can be used as a provider network interface by Neutron ([OpenStack Open Source Cloud Computing Software » Message: [openstack-dev] Problem when deploying Openstack with Kolla ](https://lists.openstack.org/pipermail/openstack-dev/2018-May/130703.html#:~:text=%3E%20,enp0s9)).

Create the Netplan YAML file `/etc/netplan/01-netcfg.yaml` (with `nano` editor): 

```yaml
network:
  version: 2
  renderer: networkd
  ethernets:
    ens32:
      dhcp4: true
    ens33:
      dhcp4: false
      addresses: [10.0.0.11/24]
      routes: []          # no default route via enp0s8
      nameservers:
        addresses: [8.8.8.8, 8.8.4.4]   # use public DNS for general name resolution
    ens34:
      dhcp4: false        # no IP (Neutron will use this interface)
      optional: true
```

Apply the Netplan config: 

```bash
sudo netplan apply
sudo rm -f /etc/netplan/50-cloud-init.yaml
```

After this, you should be able to SSH to the VM at 10.0.0.11 from the host, and the VM should still have internet via enp0s3:

```bash
ssh openstack@10.0.0.11
```

If you reinstall you may need to remove old SSH host keys on your Windows host:

```bash
ssh-keygen -R 10.0.0.11
```

Enable and verify IP forwarding:

```bash
echo "net.ipv4.ip_forward = 1" | sudo tee -a /etc/sysctl.conf
sudo sysctl -p
sudo sysctl net.ipv4.ip_forward
```

**2.3 Disable Cloud-Init Network Management:** Ubuntu cloud images may use **cloud-init** to manage network config. Since we manually configured networking, we can disable cloud-init to prevent it from overwriting our settings on reboot:

```bash
sudo touch /etc/cloud/cloud-init.disabled
```

This ensures cloud-init will not reset our network or SSH keys on reboot.

**2.4 Disable Automatic Updates:** Ubuntu enables unattended upgrades by default. We disable this to avoid package updates interfering with our OpenStack deployment. You can either reconfigure or remove the unattended-upgrades package ([apt - Disable automatic updates in 22.04.1 - Ask Ubuntu](https://askubuntu.com/questions/1437070/disable-automatic-updates-in-22-04-1#:~:text=Two%20additional%20ways%20to%20disable,unattended%2Fautomatic%20updates%2Fupgrades)):

```bash
sudo apt remove -y unattended-upgrades
```

This stops automatic package updates from running in the background ([apt - Disable automatic updates in 22.04.1 - Ask Ubuntu](https://askubuntu.com/questions/1437070/disable-automatic-updates-in-22-04-1#:~:text=sudo%20dpkg)). (You will manually update as needed.)

**2.5 Update APT and Install Basics:** Update system packages and install required utilities:

```bash
sudo apt update && sudo apt upgrade -y
sudo apt install -y bridge-utils cpu-checker libvirt-clients libvirt-daemon qemu-kvm
sudo apt install -y python3-dev python3-venv git libffi-dev gcc libssl-dev libdbus-glib-1-dev vim net-tools htop
```

These include Python tools and editors.

**2.6 Prepare the Second Disk for Cinder LVM:** The 120GB disk (`/dev/sdb` in the VM) will be used for Cinder volumes via LVM. Partitioning is optional – we can use the whole disk as a physical volume. Set up LVM:

```bash
# Create LVM physical volume
sudo pvcreate /dev/sdb
# Create a volume group named cinder-volumes
sudo vgcreate cinder-volumes /dev/sdb
```

Kolla-Ansible expects a volume group **cinder-volumes** to exist for the LVM backend. **Note:** This will erase any existing data on /dev/sdb.

Verify volume group:

```bash
sudo vgs
```

You should see **cinder-volumes** listed.

Also, Ubuntu requires a special filesystem **configfs** to be mounted for iSCSI to function inside containers. Mount it now and persist it (this allows the Cinder volume service to export volumes via iSCSI):

```bash
echo "configfs /sys/kernel/config configfs defaults 0 0" | sudo tee -a /etc/fstab
sudo mount -a
```

This mounts the configfs at `/sys/kernel/config` (needed by the iSCSI daemon) ([Cinder - Block storage — kolla-ansible 19.1.0.dev183 documentation](https://docs.openstack.org/kolla-ansible/latest/reference/storage/cinder-guide.html#:~:text=For%20Ubuntu%20and%20LVM2%2FiSCSI%C2%B6)).

**2.7 Install Required System Dependencies:** Kolla-Ansible will manage most dependencies, but we ensure a few are present:

- **Python DBus library:** The Kolla-Ansible playbooks use `dbus` for systemd interactions, which requires the `python3-dbus` package on the host. Install it now to avoid precheck errors ([Bug #2021585 “Bootstrap-server misses dbus” : Bugs : ansible-collection-kolla](https://bugs.launchpad.net/bugs/2021585#:~:text=kolla,with%20prechecks%20and%20deployment%20itself)):

```bash
sudo apt install -y python3-dbus
```

With the base system configured, reboot to ensure network and fstab changes take effect, then proceed as the `openstack` user for the next steps.

Consider shuting down the VM now to make a snapshot allowing easy restart of LAB installation without a need of full reinstall.

## 3. Install Kolla-Ansible and Ansible

We will use Kolla-Ansible (OpenStack official deployment tool) to deploy OpenStack in Docker containers. Kolla-Ansible is a Python package that can be installed via pip. 
Create a Python virtual environment:

```bash
cd ~
python3 -m venv --system-site-packages ~/kolla-ansible-venv
source ~/kolla-ansible-venv/bin/activate
pip install -U pip
```

Rember to reactivate the virtual environment every time to reboot machine and login.

**3.1 Install Kolla-Ansible (2024.2):** Install the Kolla-Ansible package corresponding to OpenStack 2024.2. At the time of writing, the stable release is **kolla-ansible 19.2.0** (which targets OpenStack 2024.2):

```bash
pip install kolla-ansible==19.2.0
```

This will install Kolla-Ansible and its Python dependencies. Once done, verify the version:

```bash
kolla-ansible --version
# Should show kolla-ansible 19.2.0 (or similar)
```

**3.2 Install Ansible Galaxy Dependencies:** Kolla-Ansible requires some Ansible roles from Galaxy (for things like configuring haproxy, etc.). Install them by running:

```bash
kolla-ansible install-deps
```

This uses the Kolla-Ansible utility to download necessary Ansible roles (you may see it installing roles for **ceph**, etc.) ([How to Implement an OpenStack-Based Private Cloud with Kolla-Ansible - Part 1 - Superuser](https://superuser.openinfra.org/articles/how-to-implement-an-openstack-based-private-cloud-with-kolla-ansible-part-1/#:~:text=5)).

**3.3 Copy Configuration and Inventory Templates:** Kolla-Ansible provides example configuration files. Copy the default configuration directory to `/etc/kolla` and the inventory for all-in-one deployment:

```bash
sudo mkdir -p /etc/kolla
sudo chown $USER:$USER /etc/kolla

# Copy example config files
cp -r  ~/kolla-ansible-venv/share/kolla-ansible/etc_examples/kolla/* /etc/kolla/

# Copy all-in-one inventory to current directory for editing
cp  ~/kolla-ansible-venv/share/kolla-ansible/ansible/inventory/all-in-one .
```

Now you should have `/etc/kolla/globals.yml` and `/etc/kolla/passwords.yml` ready to edit, and an `all-in-one` inventory file in your working directory.

**3.4 Generate Passwords:** Kolla stores all service passwords in `/etc/kolla/passwords.yml`. Generate random passwords for all entries:

```bash
kolla-genpwd
```

This will fill in `passwords.yml` with random values for all OpenStack services.

## 4. Configure Kolla-Ansible (globals.yml)

Open `/etc/kolla/globals.yml` in your editor and set the following parameters to match our environment (below first line with `---`). We will disable high availability (since this is a one-node deployment) and enable the required OpenStack services. Below is a summary of key settings to change or add:

```yaml
# Configure Network Interfaces
network_interface: "ens33"
api_interface: "ens33"
neutron_external_interface: "ens34"
dns_interface: "ens33"
kolla_internal_vip_address: "10.0.0.11"
kolla_external_vip_address: "10.0.0.11"

# Configure OpenStack release and base settings
openstack_release: "2024.2"
kolla_base_distro: "ubuntu"
kolla_install_type: "source"

# Disable High Availability
enable_haproxy: "no"
enable_keepalived: "no"
enable_mariadb_proxy: "no"
enable_proxysql: "no"
enable_rabbitmq_cluster: "no"

# Enable Core OpenStack Services
enable_keystone: "yes"
enable_glance: "yes"
enable_nova: "yes"
nova_compute_virt_type: "kvm"
enable_neutron: "yes"
enable_horizon: "yes"
enable_placement: "yes"
enable_cinder: "yes"

# Enable Additional Services
enable_heat: "yes"
enable_horizon_heat: "yes"
enable_ceilometer: "yes"
enable_aodh: "yes"
enable_gnocchi: "yes"

# Configure Cinder LVM Backend
enable_cinder_backend_lvm: "yes"
cinder_volume_group: "cinder-volumes"

# Configure Designate
enable_designate: "yes"
neutron_dns_domain: "example.test."
neutron_dns_integration: "yes"
designate_backend: "bind9"
designate_ns_record: ["ns1.example.test"]
designate_enable_notifications_sink: "yes"
designate_forwarders_addresses: 
  - "8.8.8.8"
  - "8.8.4.4"
enable_horizon_designate: "yes"
```

After making these changes, save `globals.yml`.

**4.1 (Re)Check Configuration:** It’s a good idea to double-check that `/etc/kolla/globals.yml` reflects all the changes (especially `network_interface`, `neutron_external_interface`, service enables, etc.). Also verify that `all-in-one` inventory has the IP of your host if needed (in all-in-one, by default it uses `localhost`). The provided `all-in-one` inventory typically contains `localhost       ansible_connection=local` which is fine for a single-node deploy on the same machine.

## 5. Deploy OpenStack with Kolla-Ansible

With configuration in place, we can run Kolla-Ansible playbooks to set up OpenStack. We will run the following steps in order:

1. **Bootstrap Servers** – install base packages (like Docker) on the target node.  
2. **Prechecks** – verify all prerequisites are met.  
3. **Deploy** – actually deploy all containers and services.  
4. **Post-deploy** – finalize the deployment (e.g., generate admin OpenRC file).  

Make sure you are in the directory containing your `all-in-one` inventory file and that your virtualenv (if used) is activated (so the `kolla-ansible` command is available). All commands should be run as the **openstack** user (with sudo where noted).

**5.1 Bootstrap the Server:** Run Kolla-Ansible bootstrap playbook:

```bash
kolla-ansible bootstrap-servers -i all-in-one
```

This will use Ansible to prepare the node. It installs Docker and configures basics like chrony for NTP, etc. Watch for any errors. This step requires sudo because it makes changes to the system.

   - *Troubleshooting:* The bootstrap process should install Docker CE. If it fails or if you prefer to install Docker manually, ensure Docker Engine (20.x or later) is installed and running. 
   - Ignore errors related to missing **ufw**
   
Also, since we’re using the Docker driver, confirm that the **Docker Python SDK** is installed for Ansible. If not already present, install it with: 

```bash
pip install docker
```

(If the prechecks later complain with *“No module named docker”*, this is the fix ([python - Kolla-ansible openstack: Docker version failure - Stack Overflow](https://stackoverflow.com/questions/59832284/kolla-ansible-openstack-docker-version-failure#:~:text=docker,)).)

Fix the Python `requests` module version to avoid `prechecks` issues:

```bash
pip uninstall requests
pip install requests==2.31.0
```

**5.2 Run Pre-Deployment Checks:** Next, run Kolla-Ansible prechecks to validate the configuration:

```bash
kolla-ansible prechecks -i all-in-one
```

This will do a series of checks (disk space, kernel parameters, config sanity, etc.) Ensure it finishes with “SUCCESS”. If any errors arise, address them:

   - **Common precheck issues:** Missing Python modules (like dbus or docker) or missing volume group. For instance, if you forgot to install `python3-dbus`, the precheck will fail at *"Checking dbus-python package"* ([Bug #2021585 “Bootstrap-server misses dbus” : Bugs : ansible-collection-kolla](https://bugs.launchpad.net/bugs/2021585#:~:text=kolla,with%20prechecks%20and%20deployment%20itself)). Make sure `python3-dbus` is installed on the host (as we did in step 2.7). If the Cinder LVM backend is enabled but no volume group is found, prechecks will warn you. We created `cinder-volumes` VG already, so that should pass. 

   - If any **“FAILED”** status appears, read the message and fix the issue before proceeding.

**5.3 Deploy OpenStack Services:** Now run the main deploy playbook:

```bash
kolla-ansible deploy -i all-in-one
```

This step will take some time. Kolla-Ansible will pull Docker images for all enabled services and start the containers with proper configurations. All OpenStack services (Keystone, Glance, Nova, Neutron, etc.) will be set up as Docker containers. You will see Ansible output for each role/service being deployed ([How to Implement an OpenStack-Based Private Cloud with Kolla-Ansible - Part 1 - Superuser](https://superuser.openinfra.org/articles/how-to-implement-an-openstack-based-private-cloud-with-kolla-ansible-part-1/#:~:text=3,containers%20for%20each%20OpenStack%20service)). 

Expect this to run for 10-30 minutes depending on internet speed and VM performance (as it has to download many container images and start them). 

Once completed, you should see a message indicating the playbook run is finished without fatal errors. You can check running containers with `sudo docker ps` – you should see containers for keystone, mysql, rabbitmq, horizon, neutron, nova, etc. (Do not worry if you don’t immediately see all; some containers exit after doing one-time setup, e.g., kolla_setup_* containers.)

**5.4 Post-Deployment Tasks:** After a successful deploy, run the post-deploy playbook to finalize setup:

```bash
kolla-ansible post-deploy -i all-in-one
```

This will generate an **admin OpenStack RC file** (`/etc/kolla/admin-openrc.sh`) containing environment variables to use the OpenStack CLI ([Kolla Ansible Workshop - HackMD](https://hackmd.io/s/BycDsf4OZ#:~:text=%60external_interface%20enp3s0f0%60%20%21%5B%5D%28https%3A%2F%2Fi.imgur.com%2FaQbEld4.jpg%29%20%60%20,https%3A%2F%2Fi.imgur.com%2FvV7zF6L.png%29%20OpenStack%20%E8%AA%8D%E8%AD%89%E8%B3%87%E8%A8%8A%E8%AB%8B%E5%8F%83%E8%80%83%20%60%2Fetc%2Fkolla%2Fadmin)). It may also set up default networks if that is configured (though by default, Kolla doesn’t create external network – we will do that manually).

Once this finishes, source the admin credentials:

```bash
source /etc/kolla/admin-openrc.sh
```

Install OpenStack CLI Client with **pip**:

```bash
pip install python-openstackclient python-designateclient
```

Now you can use the `openstack` CLI. For example, try `openstack service list` to see all registered OpenStack services. This verifies Keystone is working and services are registered (you should see entries for identity, compute, network, etc.).

## 6. Post-Deployment Configuration and Verification

At this point, OpenStack should be up and running. We’ll do a few additional configurations for Designate and Neutron, and verify access to Horizon and other services.

**6.1 Verify Horizon Dashboard:** The Horizon container should be running and bound to the management IP (10.0.0.11) on port 80 by default. From your host machine, open a browser and navigate to **http://10.0.0.11/**. You should see the OpenStack Dashboard login page ([How to Implement an OpenStack-Based Private Cloud with Kolla-Ansible - Part 1 - Superuser](https://superuser.openinfra.org/articles/how-to-implement-an-openstack-based-private-cloud-with-kolla-ansible-part-1/#:~:text=within%20the%20OpenStack%20environment)). Log in with the **admin** account. The password for admin can be found in `/etc/kolla/passwords.yml` (look for `keystone_admin_password`). Alternatively, since we sourced the `admin-openrc.sh`, the password is available as the environment variable `$OS_PASSWORD`. 

  - Username: `admin`  
  - Password: (from `admin-openrc.sh` or `passwords.yml`)  

  > **Tip:** OpenStack generates a random admin password when you ran `kolla-genpwd`. To retrieve it, you can run: `grep keystone_admin_password /etc/kolla/passwords.yml`. Use that value to log in.

After logging in, you should see the Horizon dashboard. This confirms that Keystone, Horizon, and their integration are functioning.

**6.2 Create External Network and Router:** Kolla-Ansible does not automatically create the external provider network or a default project network. We will create an external network to represent the 192.168.233.0/24 provider network on `enp0s9`, so that floating IPs can be allocated. Execute the following OpenStack CLI commands (as admin):

Open **Virtual Network Editor** and click on **Change Settings**, approve the action. Select **VMnet8**, the NAT network and click **NAT Settings**, read Gateway IP (like 192.168.233.2) and use it in **--gateway** option when creating **provider-net-subnet**. This is required to ensure correct outgoing traffic routing.

```bash
# 1. Create an external provider network (flat network on physnet1 by default in Kolla)
openstack network create --share --external --provider-network-type flat --provider-physical-network physnet1 provider-net

# 2. Create a subnet on this network (using VMware WOrkstaion NAT network address range: 192.168.233.0/24)
openstack subnet create --network provider-net --allocation-pool start=192.168.233.100,end=192.168.233.127 --dns-nameserver 8.8.8.8 --gateway 192.168.233.2 --subnet-range 192.168.233.0/24 provider-net-subnet
```

We named the external network **provider-net** and mapped it to `physnet1` flat network (Kolla by default maps `neutron_external_interface` to `physnet1`). We allocated a portion of the subnet for floating IPs (100-127) and set the gateway to 192.168.233.1 ([Provider network — Installation Guide  documentation](https://docs.openstack.org/install-guide/launch-instance-networks-provider.html#:~:text=The%20provider%20network%20uses%20203,4%20as%20a%20DNS%20resolver)) (the NAT network’s gateway). We also specified a DNS resolver (8.8.8.8) for instances.

Next, create an internal (tenant) network for testing (or use Horizon to do this):

```bash
# Create a demo project network (or you can use the admin project)
openstack network create demo-net
openstack subnet create --network demo-net --subnet-range 10.10.10.0/24 --dns-nameserver 8.8.8.8 demo-subnet

# Create a router and connect the demo subnet to the external net
openstack router create demo-router
openstack router set demo-router --external-gateway provider-net
openstack router add subnet demo-router demo-subnet
```

This sets up a simple tenant network (10.10.10.0/24) and a router with an interface on that subnet and a gateway to the provider network. At this point, instance traffic from the demo-net can be SNATed out to the external network by the router, and you can allocate floating IPs from provider-net to instances.

**6.3 Launch a Test Instance:** Upload a test image and try booting an instance to verify Nova, Neutron, Cinder:

```bash
wget https://download.cirros-cloud.net/0.6.3/cirros-0.6.3-x86_64-disk.img
# Upload a CirrOS test image
openstack image create --file cirros-0.6.3-x86_64-disk.img --disk-format qcow2 --container-format bare cirros

# Create a flavor
openstack flavor create --ram 256 --disk 1 --vcpus 1 m1.tiny

# Boot an instance on the demo-net
openstack server create --flavor m1.tiny --image cirros --network demo-net test-vm
```

Once the instance is ACTIVE, allocate a floating IP:

```bash
openstack floating ip create provider-net
openstack server add floating ip test-vm <floating_ip_address>
```

Use the floating IP assigned (which will be in the 192.168.233.0/24 range, e.g., 192.168.233.101 or similar) to test connectivity. Because the NAT Network is used, you should be able to directly ping this IP not only from your Ubuntu VM, as well as from the Windows host.

**6.4 Verify Cinder LVM and Volume Backup:** In Horizon or CLI, create a volume and attach it to the instance, and also test the volume backup:

- Create a volume (e.g., 1 GB) and ensure it becomes **Available** ([Cinder - Block storage — kolla-ansible 19.1.0.dev183 documentation](https://docs.openstack.org/kolla-ansible/latest/reference/storage/cinder-guide.html#:~:text=Create%20a%20volume%20as%20follows%3A)). This volume will be backed by the LVM volume group on `/dev/sdb`. Check `sudo lvs` on the host to see the logical volume created.
- Attach the volume to the `test-vm` instance and verify inside the VM that a new disk (e.g., /dev/vdb) appears.
- If **Cinder Backup** is enabled, you can create a volume backup (which by default stores backups in Swift or local filesystem depending on config – Kolla might use Swift if enabled, or just store in the same volume group). Ensure the backup operation completes, indicating the cinder-backup service is functional.

**6.5 Configure and Test Designate (DNS):** Since we enabled Designate, let’s set it up:

- Ensure the **Designate central** and **bind9** containers are running. The bind9 DNS service should be listening on the `dns_interface` (10.0.0.11). We configured it to forward unknown queries to 8.8.8.8, so it can resolve external domains as well.
- Create a DNS zone in Designate for the domain we chose (example.org):

```bash
openstack zone create --email admin@example.local example.local.
```

  This creates a DNS zone that Designate will manage. The trailing dot in the zone name is important. For example, if the command returns a zone ID, note it.

- Now we can configure Neutron to integrate with Designate. We already set `neutron_dns_domain = example.local.` in globals, which means any port/instance with a DNS name set will get a DNS record in the **example.local** zone. Test this by booting a new instance:

```bash
openstack server create --flavor m1.tiny --image cirros --network demo-net  dns-test-vm
```

  If everything is configured, once this VM is ACTIVE, Designate should have created a DNS record **testvm.example.org** pointing to the VM’s fixed IP. You can verify with:

  ```bash
  openstack recordset list example.local.
  ```

  You should see an A record for *testvm*. Also, from within the demo-net (or via the VM’s console), try querying the DNS:
  
  ```bash
  dig @10.0.0.11 testvm.example.local
  ```
  
  It should return the VM’s IP. Likewise, try resolving an external name via our Bind9:
  
  ```bash
  dig @10.0.0.11 openstack.local
  ```
  
  This should succeed (Bind9 forwards the query to 8.8.8.8 as configured, since it’s not authoritative for openstack.org). This confirms Designate and our local DNS forwarding are working.

**6.6 Telemetry (Ceilometer/Gnocchi/Aodh):** The telemetry services (if enabled) will collect metrics and allow setting alarms:

- Gnocchi should be running (as a metric storage). Ceilometer agents will publish metrics like CPU utilization, etc. to Gnocchi. You can verify Gnocchi by listing metric status (beyond scope for this guide, but ensure the containers are running: `gnocchi-api`, `gnocchi-metricd`).
- Aodh allows creating alarms on these metrics. You can test this by creating an alarm (using openstack CLI or Horizon’s Alarming panel) on an instance metric and verifying it triggers.

All OpenStack services specified are now deployed and should be functional. You can manage the cloud via Horizon or CLI. 

## 7. Conclusion

You have a fully functional OpenStack 2024.2 single-node cloud running in VirtualBox. All major components (Keystone, Glance, Nova, Neutron, Cinder, Heat, Ceilometer, Gnocchi, Aodh, Designate, Horizon) are installed in Docker containers via Kolla-Ansible. The Neutron Linuxbridge agent is used for networking, with `enp0s9` serving as a provider network for external connectivity (192.168.233.0/24 Floating IPs). Designate is integrated with Neutron to provide DNS names for instances in the **example.org** domain, backed by a local Bind9 which forwards external queries to 8.8.8.8. Cinder uses LVM on the second virtual disk for volume storage, and the cinder-backup service is enabled for volume backups. 

This lab setup can be used for testing OpenStack features in a contained environment. Since no high availability is configured, it’s not for production use, but it’s perfect for learning and experimentation. 

Feel free to explore the containers (`docker ps -a`) and logs in `/var/log/kolla/` to troubleshoot or understand the deployment. You can now launch instances, attach volumes, allocate floating IPs, experiment with Heat templates, monitor metrics in Gnocchi, and use Designate to resolve instance hostnames.

Happy OpenStacking!