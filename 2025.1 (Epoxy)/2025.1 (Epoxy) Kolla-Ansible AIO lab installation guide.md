# OpenStack 2025.1 All-in-One Installation with Kolla-Ansible on Ubuntu 24.04 LTS

This guide walks through deploying OpenStack **2025.1** (stable/Epoxy release) on a single Ubuntu 24.04 VM using **Kolla-Ansible**. We will set up an All-In-One (AIO) environment in WMware Workstation Pro 17 with one controller/compute node. All OpenStack services will run in Docker containers orchestrated by Kolla-Ansible. The configuration includes Designate for DNS service with a local Bind9 server.

## 1. Virtual Machine Setup and Networking

Consider disabling Hyper-V on your Windows machine and upgrading VMware Workstation Pro to latest release - this should remove CPU race conditions between virtualization engines.
Type `Turn Windows features on or off` into Windows Search Bar, uncheck **Virtual Machine Platform** and **Windows Hypervisor Platform**, confirm and reboot the system. 
Disable **Core Isolation** in **Windows Security**, if this is allowed in your PC.

Before installation, verify configuration of **Host-only Network** VMware Workstation Pro **Virtual Network Editor**, create networks if necessary:

- **Host-only Network**: Host-only Network, IPv4 Address 10.0.0.1, IPv4 Network Mask 255.255.255.0, DHCP Server disabled.

Create a VM with the following specs and network layout:

- in **New Virtual Machine Wizard** select **Custom (advanced)**
- in **Guest Operating System Installation** choose **Installer disk image file (iso)** and select Ubuntu server 24.04.x ISO image
- **Virtual machine name:** `os-epoxy-aio`
- Number of processor set to 8 and number of cores per processor set 1
- Set Memory for the Virtual Machine to exactly 16 GB
- in Network Type select **Use network address translation (NAT)**
- Select recommended (default) I/O Controller Type and DIsk Type, choose **Create a new virtual disk** and set Maxim disk size to 100GB, choose **Store virtual disk as a single file**
- Review settings and uncheck **Power on this virtual machine after creation** and click **Finish**

Modify Virtual Machine Hardware Settings by clicking **Edit virtual machine settings** and:

- click on **Processors** and check all options in **Virtualization engine** to enable *Nestd Virtualization*
- Add **Hard Disk**: choose recommended Disk Type, choose **Create a new virtual disk**, set size to 120GB and choose **Store virtual disk as a single file**
- Add **Network Adapter**: choose **Custom: Specific virtual network** and select **VMnet1 (Host-only)**, which you have just created
- Add another **Network Adapter**: choose **NAT: Used to share the host's IP address**
- Remove **USB Controller** and **Sound card**

Start the `os-epoxy-aio` VM and perform a **minimized** installation of Ubuntu Server 24.04 on the VM, use `English` as main language. Install system on 100GB disk (!), LVM on root disk is not required for the LAB purposes. Configure only first Network Interface with DHCP only, all other Network Interface will be configured later. Configure default user with **`openstack`** username and password of your choice. The course assumes this username in exercises. Set hostname to `controller`. Check `Install OpenSSH server` option, it's required for the LAB. 
After installation and reboot, login as `openstack` and configure the system as follows:

**2.1 Create OpenStack User with Sudo:** Give this user password-less sudo privileges for convenience:

```bash
echo "openstack ALL=(ALL:ALL) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/openstack
```

**2.2 Configure Network Interfaces:** Ubuntu 24.04 uses Netplan for networking. Edit Netplan config (e.g. `/etc/netplan/01-netcfg.yaml`) to set up the NICs:

- `ens32`: DHCP (for NAT internet access).
- `ens33`: Static IP **10.0.0.11/24** (no gateway on this interface, since the host-only network is non-routable).
- `ens34`: Leave unconfigured (no IP) so it can be used as a provider network interface by Neutron.

Create the Netplan YAML file `/etc/netplan/01-netcfg.yaml`: 

```bash
cat << EOF | sudo tee /etc/netplan/01-netcfg.yaml
network:
  version: 2
  renderer: networkd
  ethernets:
    ens32:
      dhcp4: true
    ens33:
      dhcp4: false
      addresses: [ 10.0.0.11/24 ]
      routes: []          # no default route via enp0s8
      nameservers:
        addresses: [ 8.8.8.8 ]   # use public DNS for general name resolution
    ens34:
      dhcp4: false        # no IP (Neutron will use this interface)
      optional: true
EOF
```

Apply the Netplan config: 

```bash
sudo netplan apply
sudo rm -f /etc/netplan/50-cloud-init.yaml
```

After this, you should be able to SSH to the VM at 10.0.0.11 from the host, and the VM should still have internet via enp0s3:

```bash
ssh openstack@10.0.0.11
```

If you reinstall you may need to remove old SSH host keys on your Windows host:

```bash
ssh-keygen -R 10.0.0.11
```

Enable and verify IP forwarding:

```bash
echo "net.ipv4.ip_forward = 1" | sudo tee -a /etc/sysctl.conf
sudo sysctl -p
sudo sysctl net.ipv4.ip_forward
```

**2.3 Disable Cloud-Init Network Management:** Ubuntu cloud images may use **cloud-init** to manage network config. Since we manually configured networking, we can disable cloud-init to prevent it from overwriting our settings on reboot:

```bash
sudo touch /etc/cloud/cloud-init.disabled
```

This ensures cloud-init will not reset our network or SSH keys on reboot.

**2.4 Disable Automatic Updates:** Ubuntu enables unattended upgrades by default. We disable this to avoid package updates interfering with our OpenStack deployment. You can either reconfigure or remove the unattended-upgrades package:

```bash
sudo apt remove -y unattended-upgrades
```

This stops automatic package updates from running in the background. (You will manually update as needed.)

**2.5 Update APT and Install Basics:** Update system packages and install required utilities:

```bash
sudo apt update && sudo apt upgrade -y
sudo apt install -y bridge-utils cpu-checker libvirt-clients libvirt-daemon qemu-kvm
sudo apt install -y python3-dev python3-venv git libffi-dev gcc libssl-dev libdbus-glib-1-dev vim nano net-tools htop dnsutils yq
```

These include Python tools and editors.

**2.6 Prepare the Second Disk for Cinder LVM:** The 120GB disk (`/dev/sdb` in the VM) will be used for Cinder volumes gropup `cinder-volumes` via LVM. Set up LVM:

```bash
# Initialize the disk as a PV
sudo pvcreate /dev/sdb

# Verify PV creation (optional)
sudo pvs

# Create the first VG using the first partition
sudo vgcreate cinder-volumes /dev/sdb
```

Kolla-Ansible expects a volume group **cinder-volumes** to exist for the LVM backend. **Note:** This will erase any existing data on /dev/sdb.

Verify volume group:

```bash
sudo vgs
```

You should see **cinder-volumes** listed.

Also, Ubuntu requires a special filesystem **configfs** to be mounted for iSCSI to function inside containers. Mount it now and persist it (this allows the Cinder volume service to export volumes via iSCSI):

```bash
echo "configfs /sys/kernel/config configfs defaults 0 0" | sudo tee -a /etc/fstab
sudo mount -a
```

This mounts the configfs at `/sys/kernel/config` (needed by the iSCSI daemon).

**2.7 Install Required System Dependencies:** Kolla-Ansible will manage most dependencies, but we ensure a few are present:

- **Python DBus library:** The Kolla-Ansible playbooks use `dbus` for systemd interactions, which requires the `python3-dbus` package on the host. Install it now to avoid precheck errors:

```bash
sudo apt install -y python3-dbus
```

With the base system configured, reboot to ensure network and fstab changes take effect, then proceed as the `openstack` user for the next steps.

Consider shuting down the VM now to make a snapshot allowing easy restart of LAB installation without a need of full reinstall.

## 3. Install Kolla-Ansible and Ansible

We will use Kolla-Ansible (OpenStack official deployment tool) to deploy OpenStack in Docker containers. Kolla-Ansible is a Python package that can be installed via pip. 
Create a Python virtual environment:

```bash
cd ~
python3 -m venv --system-site-packages ~/openstack-venv
source ~/openstack-venv/bin/activate
pip install -U pip
```

We want the Virtual Environment to get activate with each subsequent login, so let's add activation of `openstack-venv` to `~/.bashrc` :

```bash
echo "source ~/openstack-venv/bin/activate" >> ~/.bashrc
```

**3.1 Install Kolla-Ansible (2025.1):** Install the Kolla-Ansible package corresponding to OpenStack 2025.1. At the time of writing, the stable release is **stable/2025.1** (which targets OpenStack 2025.1):

```bash
pip install git+https://opendev.org/openstack/kolla-ansible@stable/2025.1
```

This will install Kolla-Ansible and its Python dependencies. Once done, verify the version:

```bash
kolla-ansible --version
# Should show kolla-ansible 19.4.0 (or similar)
```

**3.2 Install Ansible Galaxy Dependencies:** Kolla-Ansible requires some Ansible roles from Galaxy (for things like configuring haproxy, etc.). Install them by running:

```bash
kolla-ansible install-deps
```

**3.3 Copy Configuration and Inventory Templates:** Kolla-Ansible provides example configuration files. Copy the default configuration directory to `/etc/kolla` and the inventory for all-in-one deployment:

```bash
sudo mkdir -p /etc/kolla
sudo chown $USER:$USER /etc/kolla

# Copy example config files
cp -r  ~/openstack-venv/share/kolla-ansible/etc_examples/kolla/* /etc/kolla/

# Copy all-in-one inventory to current directory for editing
cp  ~/openstack-venv/share/kolla-ansible/ansible/inventory/all-in-one .
```

Now you should have `/etc/kolla/globals.yml` and `/etc/kolla/passwords.yml` ready to edit, and an `all-in-one` inventory file in your working directory.

**3.4 Generate Passwords:** Kolla stores all service passwords in `/etc/kolla/passwords.yml`. Generate random passwords for all entries:

```bash
kolla-genpwd
```

This will fill in `passwords.yml` with random values for all OpenStack services. Now, we set the `admin` user password to `openstack`:

```bash
sudo yq -i -y '.keystone_admin_password = "openstack"' /etc/kolla/passwords.yml
```

## 4. Configure Kolla-Ansible (globals.yml)

Set following parameters in `/etc/kolla/globals.yml` to match our environment. We will disable high availability (since this is a one-node deployment) and enable the required OpenStack services:

```bash
cat EOF | tee -a /etc/kolla/globals.yml
# ---------------------------------------------------
#
# OpenStack Epoxy All-in-One Lab deployment configuration
#
# Configure Network Interfaces
network_interface: "ens33"
api_interface: "ens33"
neutron_external_interface: "ens34"
dns_interface: "ens33"
kolla_internal_vip_address: "10.0.0.11"
kolla_external_vip_address: "10.0.0.11"

# Configure OpenStack release and base settings
openstack_release: "2025.1"
kolla_base_distro: "ubuntu"
kolla_install_type: "source"

# Disable High Availability
enable_haproxy: "no"
enable_keepalived: "no"
enable_mariadb_proxy: "no"
enable_proxysql: "no"
enable_rabbitmq_cluster: "no"

# Enable Core OpenStack Services
enable_keystone: "yes"
enable_glance: "yes"
enable_nova: "yes"
nova_compute_virt_type: "kvm"
enable_neutron: "yes"
enable_horizon: "yes"
enable_placement: "yes"
enable_cinder: "yes"

# Enable Additional Services
enable_heat: "yes"
enable_horizon_heat: "yes"
enable_skyline: "yes"

# Configure Cinder LVM Backend
enable_cinder_backend_lvm: "yes"

# Configure Designate
enable_designate: "yes"
neutron_dns_domain: "example.test."
neutron_dns_integration: "yes"
designate_backend: "bind9"
designate_ns_record: ["ns1.example.test"]
designate_enable_notifications_sink: "yes"
designate_forwarders_addresses: "8.8.8.8"
enable_horizon_designate: "yes"
EOF
```

After making these changes, save `globals.yml`.

**4.1 (Re)Check Configuration:** It’s a good idea to double-check that `/etc/kolla/globals.yml` reflects all the changes (especially `network_interface`, `neutron_external_interface`, service enables, etc.). Also verify that `all-in-one` inventory has the IP of your host if needed (in all-in-one, by default it uses `localhost`). The provided `all-in-one` inventory typically contains `localhost       ansible_connection=local` which is fine for a single-node deploy on the same machine.

## 5. Deploy OpenStack with Kolla-Ansible

With configuration in place, we can run Kolla-Ansible playbooks to set up OpenStack. We will run the following steps in order:

1. **Bootstrap Servers** – install base packages (like Docker) on the target node.  
2. **Prechecks** – verify all prerequisites are met.  
3. **Deploy** – actually deploy all containers and services.  
4. **Post-deploy** – finalize the deployment (e.g., generate admin OpenRC file).  

Make sure you are in the directory containing your `all-in-one` inventory file and that your virtualenv (if used) is activated (so the `kolla-ansible` command is available). All commands should be run as the **openstack** user (with sudo where noted).

**5.1 Bootstrap the Server:** Run Kolla-Ansible bootstrap playbook:

```bash
kolla-ansible bootstrap-servers -i all-in-one
```

**5.2 Run Pre-Deployment Checks:** Next, run Kolla-Ansible prechecks to validate the configuration:

```bash
kolla-ansible prechecks -i all-in-one
```

This will do a series of checks (disk space, kernel parameters, config sanity, etc.) Ensure it finishes with “SUCCESS”. If any errors arise, address them:

   - **Common precheck issues:** Missing Python modules (like dbus or docker) or missing volume group. For instance, if you forgot to install `python3-dbus`, the precheck will fail at *"Checking dbus-python package"*. Make sure `python3-dbus` is installed on the host (as we did in step 2.7). If the Cinder LVM backend is enabled but no volume group is found, prechecks will warn you. We created `cinder-volumes` VG already, so that should pass. 

   - If any **“FAILED”** status appears, read the message and fix the issue before proceeding.

**5.3 Deploy OpenStack Services:** Now run the main deploy playbook:

```bash
kolla-ansible deploy -i all-in-one
```

This step will take some time. Kolla-Ansible will pull Docker images for all enabled services and start the containers with proper configurations. All OpenStack services (Keystone, Glance, Nova, Neutron, etc.) will be set up as Docker containers. You will see Ansible output for each role/service being deployed. 

Expect this to run for 10-30 minutes depending on internet speed and VM performance (as it has to download many container images and start them). 

Once completed, you should see a message indicating the playbook run is finished without fatal errors. You can check running containers with `sudo docker ps` – you should see containers for keystone, mysql, rabbitmq, horizon, neutron, nova, etc. (Do not worry if you don’t immediately see all; some containers exit after doing one-time setup, e.g., kolla_setup_* containers.)

**5.4 Post-Deployment Tasks:** After a successful deploy, run the post-deploy playbook to finalize setup:

```bash
kolla-ansible post-deploy -i all-in-one
```

This will generate an **admin OpenStack RC file** (`/etc/kolla/admin-openrc.sh`) containing environment variables to use the OpenStack CLI . It may also set up default networks if that is configured (though by default, Kolla doesn’t create external network – we will do that manually).

Install OpenStack CLI Client with **pip**:

```bash
pip install python-openstackclient python-designateclient
```

Now you can use the `openstack` CLI. For example, try `openstack service list` to see all registered OpenStack services. This verifies Keystone is working and services are registered (you should see entries for identity, compute, network, etc.).

## 6. Post-Deployment Configuration and Verification

At this point, OpenStack should be up and running. We’ll do a few additional configurations for Designate and Neutron, and verify access to Horizon and other services.

**6.1 Verify Horizon Dashboard:** The Horizon container should be running and bound to the management IP (10.0.0.11) on port 80 by default. From your host machine, open a browser and navigate to **http://10.0.0.11/**. You should see the OpenStack Dashboard login page. Log in with the **admin** account. The password for admin can be found in `/etc/kolla/passwords.yml` (look for `keystone_admin_password`). Alternatively, since we sourced the `admin-openrc.sh`, the password is available as the environment variable `$OS_PASSWORD`. 

- Username: `admin`  
- Password: (run `grep OS_PASSWORD /etc/kolla/admin-openrc.sh` to display password, copy it without quote signs)  

After logging in, you should see the Horizon dashboard. This confirms that Keystone, Horizon, and their integration are functioning.

Modern OpenStack dashboard **Skyline** is also available at [http://10.0.0.11:9999](http://10.0.0.11:9999), login with the same **admin** credentials.

Source the **admin** credentials to make `openstack` command work:

```bash
source /etc/kolla/admin-openrc.sh
```

**6.2 Configure Designate:** Since we enabled Designate, let’s set it up:

- Ensure the **Designate central** and **bind9** containers are running. The Bind9 DNS service should be listening on the `dns_interface` (10.0.0.11). We configured it to forward unknown queries to 8.8.8.8, so it can resolve external domains as well.
- Create a DNS zone in Designate for the domain we chose (`example.test.`):

```bash
openstack zone create --email admin@example.test example.test.
```

This creates a DNS zone that Designate will manage. The trailing dot in the zone name is important.
The command returns a zone ID, note it.
We must configure **Designate Sink** to enable **Nova** sending Instance creation and deletion events to **Designate** in order to create DNS and remove DNS records for the Instance:

```bash
# Save ID of the zone example.test. to ZONE_ID shell variable
ZONE_ID=$(openstack zone show -f value -c id example.test.)

# Create a subdirectory and configuration file
mkdir -p /etc/kolla/config/designate/
cat << EOF > /etc/kolla/config/designate/designate-sink.conf
[handler:nova_fixed]
zone_id = $ZONE_ID
[handler:neutron_floatingip]
zone_id = $ZONE_ID
EOF
```

Reconfigure **Designate**, **Neutron** and **Nova** container to implement the change:

```bash
kolla-ansible reconfigure -i all-in-one --tags designate,neutron,nova
```

Update Name Server configuration for **Management Network** in Ubuntu VM:

```bash
cat << EOF | sudo tee /etc/netplan/01-netcfg.yaml
network:
  version: 2
  renderer: networkd
  ethernets:
    ens32:
      dhcp4: true
    ens33:
      dhcp4: false
      addresses: [ 10.0.0.11/24 ]
      routes: []          # no default route via enp0s8
      nameservers:
        addresses: [ 10.0.0.11 ]
        search: [ example.test ]
    ens34:
      dhcp4: false        # no IP (Neutron will use this interface)
      optional: true
EOF
```

Apply the change:

```bash
sudo netplan apply
```

**6.3 Create External Network and Router:** Kolla-Ansible does not automatically create the external provider network or a default project network. We will create an external network to represent the 192.168.233.0/24 provider network on `enp0s9`, so that floating IPs can be allocated. Execute the following OpenStack CLI commands (as admin):

Open **Virtual Network Editor** and click on **Change Settings**, approve the action. Select **VMnet8**, the NAT network and click **NAT Settings**, read Gateway IP (like 192.168.233.2) and use it in **--gateway** option when creating **provider-net-subnet**. This is required to ensure correct outgoing traffic routing.

```bash
echo "---> Creating provider network"
openstack network create --share --external --dns-domain example.test. --provider-network-type flat --provider-physical-network physnet1 provider-net
VM_NAT_net_prefix=$(sudo ip -4 -o a show ens32 | awk '{print $4}' | cut -d '.' -f 1,2,3)
openstack subnet create --network provider-net --allocation-pool start=$VM_NAT_net_prefix.100,end=$VM_NAT_net_prefix.127 --gateway $VM_NAT_net_prefix.2 --subnet-range $VM_NAT_net_prefix.0/24 provider-net-subnet
echo "<---"
```

We named the external network **provider-net** and mapped it to `physnet1` flat network (Kolla by default maps `neutron_external_interface` to `physnet1`). We allocated a portion of the subnet for floating IPs (100-127) and set the gateway to 192.168.233.1.

Create a project **demo-project** for lab exercises and user **demo** in this project:

```bash
openstack project create --enable demo-project
openstack user create --project demo-project --password openstack --ignore-password-expiry demo
openstack role add --user demo --project demo-project member
```

User's password is set to `openstack`, which is OK for the demo Lab. Let's create an **openrc** file the user `demo` in project `demo-project`:


```bash
cat << EOF > ~/.demo-openrc.sh
export OS_PROJECT_DOMAIN_NAME='Default'
export OS_USER_DOMAIN_NAME='Default'
export OS_PROJECT_NAME='demo-project'
export OS_TENANT_NAME='demo-project'
export OS_USERNAME='demo'
export OS_PASSWORD='openstack'
export OS_AUTH_URL='http://10.0.0.11:5000'
export OS_INTERFACE='internal'
export OS_ENDPOINT_TYPE='internalURL'
export OS_IDENTITY_API_VERSION='3'
export OS_REGION_NAME='RegionOne'
export OS_AUTH_PLUGIN='password'
EOF
```

Create **flavors** as **admin** user (defore switching to **demo** user credentials), a public cirros image, and share the `example.test.` zone with demo project:

```bash
echo "---> Creating m1.tiny flavor"
openstack flavor create --ram 256 --disk 1 --vcpus 1 m1.tiny
echo "<---"

echo "---> Creating m1.small flavor"
openstack flavor create --ram 512 --disk 5 --vcpus 1 m1.small
echo "<---"

echo "---> Creating m1.standard flavor"
openstack flavor create --ram 1024 --disk 10 --vcpus 1 m1.standard
echo "<---"

echo "---> Creating a public image public-cirros"
wget https://download.cirros-cloud.net/0.6.3/cirros-0.6.3-x86_64-disk.img
openstack image create --public --file cirros-0.6.3-x86_64-disk.img --disk-format qcow2 --container-format bare public-cirros
echo "<---"

# Share the Designate zone example.test. with project demo-project using its ID
PROJECT_TENANT_ID=$(openstack project show -f value -c id demo-project)
openstack zone share create example.test. $PROJECT_TENANT_ID
```

You can now login as demo in **Horizon** console. Let's set `demo` credentials in shell session:

```bash
source ~/.demo-openrc.sh
```

Next, create an internal (tenant) network for testing (or use Horizon to do this):

```bash
echo "---> Creating demo-net network and demo-router in project demo-project"
openstack network create demo-net
openstack subnet create --network demo-net --subnet-range 10.10.10.0/24 demo-subnet
openstack router create demo-router
openstack router set demo-router --external-gateway provider-net
openstack router add subnet demo-router demo-subnet
echo "<---"
```

This sets up a simple tenant network (10.10.10.0/24) and a router with an interface on that subnet and a gateway to the provider network. At this point, instance traffic from the demo-net can be SNATed out to the external network by the router, and you can allocate floating IPs from provider-net to instances.

**6.4 Launch a Test Instance:** Upload a test image and try booting an instance to verify Nova, Neutron, Cinder:

```bash
echo "---> Creating private cirros image"
openstack image create --file cirros-0.6.3-x86_64-disk.img --disk-format qcow2 --container-format bare demo-cirros
echo "<---"

# Boot an instance on the demo-net
openstack server create --flavor m1.tiny --image demo-cirros --network demo-net test-vm
```

Once the instance is ACTIVE, allocate a floating IP:

```bash
FLOATING_IP=$(openstack floating ip create -f value -c floating_ip_address --dns-domain example.test. --dns-name test-fip provider-net)
openstack server add floating ip test-vm $FLOATING_IP
```

Create a Security Group `ssh-icmp` to allow incomming ICMP and SSH traffic:

```bash
echo "---> Creating security group allowing ingres of ICMP (ping) and SSH traffic"
openstack security group create --description 'Allows ssh and ping from any host' ssh-icmp
openstack security group rule create --ethertype IPv4 --protocol icmp --remote-ip 0.0.0.0/0 ssh-icmp
openstack security group rule create --ethertype IPv4 --protocol tcp --dst-port 22 --remote-ip 0.0.0.0/0 ssh-icmp
echo "<---"
```

Add this ecurity group to `test-vm`:

```bash
openstack server add security group test-vm ssh-icmp
```

**6.4 Test Designate (DNS):** 

We already set `neutron_dns_domain = example.test.` in globals, which means any port/instance with a DNS name set will get a DNS record in the **example.local** zone:

```bash
openstack recordset list example.test.
```

**6.5 Verify Cinder LVM:** In Horizon or CLI, create a volume and attach it to the instance:

- Create a volume (e.g., 1 GB) and ensure it becomes **Available**. This volume will be backed by the LVM volume group on `/dev/sdb` of Ubuntu VM. Check `sudo lvs` on the host to see the logical volume created.
- Attach the volume to the `test-vm` instance and verify inside the VM that a new disk (e.g., /dev/vdb) appears.

Delete test volume and test instance before proceeding.

All OpenStack services specified are now deployed and should be functional. You can manage the cloud via Horizon or CLI. 

## 7. Conclusion

You have a fully functional OpenStack 2025.1 single-node cloud running in VMware Workstation Pro. All major components (Keystone, Glance, Nova, Neutron, Cinder, Heat, Designate, Horizon) are installed in Docker containers via Kolla-Ansible. Designate is integrated with Neutron to provide DNS names for instances in the **example.org** domain, backed by a local Bind9 which forwards external queries to 8.8.8.8. Cinder uses LVM on the second virtual disk for volume storage. 

This lab setup can be used for testing OpenStack features in a contained environment. Since no high availability is configured, it’s not for production use, but it’s perfect for learning and experimentation. 

Feel free to explore the containers (`docker ps -a`) and logs in `/var/log/kolla/` to troubleshoot or understand the deployment. You can now launch instances, attach volumes, allocate floating IPs, experiment with Heat templates, and use Designate to resolve instance hostnames.

Happy OpenStacking!